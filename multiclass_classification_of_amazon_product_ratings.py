# -*- coding: utf-8 -*-
"""Multiclass Classification of Amazon Product Ratings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1laPMC5b_lKgTGTGJbBc6Ee03xqJpUg1W

# Multiclass Classification of Amazon Product Ratings: Perceptron, Logistic Regression, and Random Forest Classifier

##Classifier #1: Perceptron
"""

import autograd.numpy as np
from autograd import grad
import matplotlib.pyplot as plt
import math
import pandas as pd

from sklearn.linear_model import Perceptron
from sklearn.metrics import roc_curve
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

train_data = pd.read_csv('/content/amazon_train.csv')
test_data = pd.read_csv('/content/amazon_test.csv')

print(train_data.shape)
print(train_data.columns)
train_data.head()

from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold
cv = KFold(n_splits=5, random_state=1, shuffle=True)

from sklearn.calibration import CalibratedClassifierCV


train_features = train_data['reviewText'].fillna('') + ' ' + train_data['summary'].fillna('')
test_features = test_data['reviewText'].fillna('') + ' ' + test_data['summary'].fillna('')
train_target = train_data['overall']


def perceptron(train_data, test_data):


   x_train, x_val, y_train, y_val = train_test_split(train_data, train_target, test_size=0.2, random_state=42)


   # Create an instance of Perceptron
   perceptron_model = Perceptron()


   parameters = {'max_iter': [1000]}


   # perceptron_model CalibratedClassifierCV
   perceptron_cv = CalibratedClassifierCV(perceptron_model, cv=5)
   perceptron_cv.fit(x_train, y_train)


   perceptron_predict = perceptron_cv.predict(test_data)


   perceptron_preds_val = perceptron_cv.predict(x_val)
   perceptron_probs_val = perceptron_cv.predict_proba(x_val)
   evaluate(y_val, perceptron_preds_val, perceptron_probs_val)


   return perceptron_predict

from sklearn.svm import LinearSVC
from sklearn.metrics import roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer


tfidf_v = TfidfVectorizer()


x_train_features = tfidf_v.fit_transform(train_features.values.astype('U'))
x_test_features = tfidf_v.transform(test_features.values.astype('U'))

#Calculate accuracy
def evaluate(y_real, y_pred, y_proba):
  conf_mat = confusion_matrix(y_real, y_pred)
  accuracy = accuracy_score(y_real, y_pred)
  f= f1_score(y_real, y_pred, average='macro')
  r = roc_auc_score(y_real, y_proba, multi_class ='ovr')


  print("Evaluation Metrics Scores for this model are")
  print("Confusion Matrix is\n", conf_mat)
  print("ROC AUC Score is", r)
  print("Accuray Score is", accuracy)
  print("F1 Macro Score is", f)

perceptron_preds = perceptron(x_train_features, x_test_features)

# submit predictions
submission = pd.DataFrame({'id':test_data['id'], 'pred': perceptron_preds})
submission.to_csv('Perceptron7.csv', index=False)
submission.head()

"""##Classifier #2: Logistic Regression"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install vaderSentiment
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import autograd.numpy as np
# from autograd import grad
# import matplotlib.pyplot as plt
# import math
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import roc_curve
# from sklearn.metrics import f1_score
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import roc_auc_score
# from sklearn.metrics import confusion_matrix
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics._plot.confusion_matrix import confusion_matrix
# from sklearn.model_selection import GridSearchCV
# from scipy.sparse import hstack
# from nltk.stem import WordNetLemmatizer
# from nltk.stem import *
# from nltk import word_tokenize
# import nltk
# nltk.download('wordnet')
# nltk.download('punkt')

# read and preview data sets
train_data = pd.read_csv('/content/amazon_train.csv')
test_data = pd.read_csv('/content/amazon_test.csv')

print(train_data.shape)
print(train_data.columns)
train_data.head()

# define the target label
target = train_data['overall']

# select the data
train_data_vectorized = train_data['reviewText'].fillna('') + ' ' + train_data['summary'].fillna('')
test_data_vectorized = test_data['reviewText'].fillna('') + ' ' + test_data['summary'].fillna('')

# preprocess the data using lemmatization
lemmatizer = WordNetLemmatizer()

def lemmatize(text):
    tokens = word_tokenize(text)
    return [lemmatizer.lemmatize(token) for token in tokens]

# vectorize the text data
vectorizer = TfidfVectorizer(ngram_range=(1, 2),
                            sublinear_tf=True,
                            tokenizer=lemmatize)

train_data_vectorized = vectorizer.fit_transform(train_data_vectorized.values.astype('U'))
test_data_vectorized = vectorizer.transform(test_data_vectorized.values.astype('U'))

# split the data to prepare for cross-validation
x_train, x_test, y_train, y_test = train_test_split(train_data_vectorized, target, test_size=0.2, random_state=42)

# find optimal parameters
log_reg = LogisticRegression()
parameters = {'max_iter': [2000], 'C': [3.0]}
log_reg_model = GridSearchCV(log_reg, parameters, cv=5, scoring='f1_macro', n_jobs=-1)

# fit the model
log_reg_model.fit(x_train, y_train)

# print the optimal parameters
log_reg_model_parameters = log_reg_model.best_params_
print(log_reg_model_parameters)

# predict validation data set labels
y_pred = log_reg_model.predict(x_test)

# evaluate model based on the validation data set predictions
confusion_matrix_metric = confusion_matrix(y_test, y_pred)
roc_auc_score_metric = roc_auc_score(y_test, log_reg_model.predict_proba(x_test), multi_class='ovr')
accuracy_score_metric = accuracy_score(y_test, y_pred)
f1_macro_score_metric = f1_score(y_test, y_pred, average='macro')

print("Evaluation Metrics Scores for this model are")
print("Confusion Matrix is\n", confusion_matrix_metric)
print("ROC AUC Score is", roc_auc_score_metric)
print("Accuray Score is", accuracy_score_metric)
print("F1 Macro Score is", f1_macro_score_metric)

# predict labels
test_predictions = log_reg_model.predict(test_data_vectorized)

# submit predictions
submission = pd.DataFrame({'id': test_data['id'], 'pred': test_predictions})
submission.to_csv('LogisticRegressionModel4.csv', index=False)
submission.head()

"""##Classifier #3: Random Forest Classifier"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install vaderSentiment
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# import autograd.numpy as np
# from autograd import grad
# import matplotlib.pyplot as plt
# import math
# import pandas as pd
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import roc_curve
# from sklearn.metrics import f1_score
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import roc_auc_score
# from sklearn.metrics import confusion_matrix
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.metrics._plot.confusion_matrix import confusion_matrix
# from sklearn.model_selection import GridSearchCV
# from scipy.sparse import hstack
# from sklearn.ensemble import RandomForestClassifier
# from nltk.stem import WordNetLemmatizer
# from nltk.stem import *
# from nltk import word_tokenize
# import nltk
# nltk.download('wordnet')
# nltk.download('punkt')

# read and preview data sets
train_data = pd.read_csv('/content/amazon_train.csv')
test_data = pd.read_csv('/content/amazon_test.csv')

print(train_data.shape)
print(train_data.columns)
train_data.head()

# define the target label
target = train_data['overall']

# select the data
train_data_vectorized = train_data['reviewText'].fillna('') + ' ' + train_data['summary'].fillna('')
test_data_vectorized = test_data['reviewText'].fillna('') + ' ' + test_data['summary'].fillna('')

# preprocess the data using lemmatization
lemmatizer = WordNetLemmatizer()

def lemmatize(text):
    tokens = word_tokenize(text)
    return [lemmatizer.lemmatize(token) for token in tokens]

# vectorize the text data
vectorizer = TfidfVectorizer(ngram_range=(1, 2),
                            sublinear_tf=True,
                            tokenizer=lemmatize)

train_data_vectorized = vectorizer.fit_transform(train_data_vectorized.values.astype('U'))
test_data_vectorized = vectorizer.transform(test_data_vectorized.values.astype('U'))

# split the data to prepare for cross-validation
x_train, x_test, y_train, y_test = train_test_split(train_data_vectorized, target, test_size=0.2, random_state=42)

# find optimal parameters
rf_clf = RandomForestClassifier()
parameters = {  'n_estimators': [50, 100], 'max_depth': [25] }
rf_clf_model = GridSearchCV(rf_clf, parameters, cv=5, scoring='f1_macro', n_jobs=-1)

# fit the model
rf_clf_model.fit(x_train, y_train)

# print the optimal parameters
rf_clf_model_parameters = rf_clf_model.best_params_
print(rf_clf_model_parameters)

# predict validation data set labels
y_pred = rf_clf_model.predict(x_test)

# evaluate model based on the validation data set predictions
confusion_matrix_metric = confusion_matrix(y_test, y_pred)
roc_auc_score_metric = roc_auc_score(y_test, log_reg_model.predict_proba(x_test), multi_class='ovr')
accuracy_score_metric = accuracy_score(y_test, y_pred)
f1_macro_score_metric = f1_score(y_test, y_pred, average='macro')

print("Evaluation Metrics Scores for this model are")
print("Confusion Matrix is\n", confusion_matrix_metric)
print("ROC AUC Score is", roc_auc_score_metric)
print("Accuray Score is", accuracy_score_metric)
print("F1 Macro Score is", f1_macro_score_metric)

# predict labels
test_predictions = rf_clf_model.predict(test_data_vectorized)

# submit predictions
submission = pd.DataFrame({'id': test_data['id'], 'pred': test_predictions})
submission.to_csv('RandomForestClassifierModel2.csv', index=False)
submission.head()